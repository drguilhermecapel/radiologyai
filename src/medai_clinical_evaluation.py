"""
MedAI Clinical Evaluation - Sistema de avalia√ß√£o cl√≠nica
Implementa m√©tricas e benchmarks para avalia√ß√£o de performance cl√≠nica
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple
import logging
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

logger = logging.getLogger('MedAI.ClinicalEvaluation')

class ClinicalPerformanceEvaluator:
    """
    Avaliador de performance cl√≠nica para modelos de IA m√©dica
    Implementa m√©tricas espec√≠ficas para radiologia baseadas no scientific guide
    Sensibilidade > 95% para condi√ß√µes cr√≠ticas, > 90% para moderadas
    """
    
    def __init__(self, class_names: List[str] = None):
        self.evaluation_history = []
        self.benchmarks = {}
        self.class_names = class_names or ['normal', 'pneumonia', 'pleural_effusion', 'fracture', 'tumor']
        
        self.critical_conditions = ['pneumothorax', 'tumor', 'fracture']  # >95% sensibilidade
        self.moderate_conditions = ['pneumonia', 'pleural_effusion']      # >90% sensibilidade
        self.target_specificity = 0.90  # >90% especificidade para reduzir falsos positivos
        
        logger.info("ClinicalPerformanceEvaluator inicializado com valida√ß√£o cl√≠nica avan√ßada")
    
    def evaluate_model_performance(self, 
                                 y_true: np.ndarray, 
                                 y_pred: np.ndarray, 
                                 y_prob: Optional[np.ndarray] = None) -> Dict:
        """
        Avalia performance do modelo usando m√©tricas cl√≠nicas
        
        Args:
            y_true: Labels verdadeiros
            y_pred: Predi√ß√µes do modelo
            y_prob: Probabilidades das predi√ß√µes (opcional)
            
        Returns:
            Dicion√°rio com m√©tricas de performance
        """
        try:
            y_true = np.array(y_true).flatten()
            y_pred = np.array(y_pred).flatten()
            
            min_length = min(len(y_true), len(y_pred))
            if len(y_true) != len(y_pred):
                logger.warning(f"Inconsistent sample sizes: y_true={len(y_true)}, y_pred={len(y_pred)}. Truncating to {min_length}")
                y_true = y_true[:min_length]
                y_pred = y_pred[:min_length]
            
            metrics = {}
            
            if len(np.unique(y_true)) == 2 and len(np.unique(y_pred)) == 2:
                tp = np.sum((y_true == 1) & (y_pred == 1))
                tn = np.sum((y_true == 0) & (y_pred == 0))
                fp = np.sum((y_true == 0) & (y_pred == 1))
                fn = np.sum((y_true == 1) & (y_pred == 0))
                
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0
                
                accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
                precision = ppv
                recall = sensitivity
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
                
                metrics = {
                    'accuracy': float(accuracy),
                    'precision': float(precision),
                    'recall': float(recall),
                    'sensitivity': float(sensitivity),
                    'specificity': float(specificity),
                    'ppv': float(ppv),
                    'npv': float(npv),
                    'f1_score': float(f1),
                    'tp': int(tp),
                    'tn': int(tn),
                    'fp': int(fp),
                    'fn': int(fn)
                }
            else:
                # Multi-class classification - use sklearn metrics with consistent sample sizes
                if len(y_true) == 0 or len(y_pred) == 0:
                    logger.warning("Empty arrays provided for evaluation")
                    metrics = {
                        'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0,
                        'sensitivity': 0.0, 'specificity': 0.0
                    }
                else:
                    metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
                    metrics['precision'] = float(precision_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['recall'] = float(recall_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['f1_score'] = float(f1_score(y_true, y_pred, average='weighted', zero_division=0))
                    
                    clinical_metrics = self._calculate_clinical_metrics(y_true, y_pred)
                    metrics.update(clinical_metrics)
                
                # Calculate average sensitivity and specificity
                sensitivity_values = [v for k, v in clinical_metrics.items() if 'sensitivity' in k]
                specificity_values = [v for k, v in clinical_metrics.items() if 'specificity' in k]
                
                metrics['sensitivity'] = float(np.mean(sensitivity_values)) if sensitivity_values else 0.0
                metrics['specificity'] = float(np.mean(specificity_values)) if specificity_values else 0.0
            
            cm = confusion_matrix(y_true, y_pred)
            metrics['confusion_matrix'] = cm.tolist()
            
            if y_prob is not None and len(y_true) > 0:
                try:
                    if hasattr(y_prob, 'shape') and len(y_prob.shape) > 1:
                        if y_prob.shape[0] != len(y_true):
                            min_samples = min(y_prob.shape[0], len(y_true))
                            y_prob = y_prob[:min_samples]
                            y_true = y_true[:min_samples]
                            y_pred = y_pred[:min_samples]
                    else:
                        if len(y_prob) != len(y_true):
                            min_samples = min(len(y_prob), len(y_true))
                            y_prob = y_prob[:min_samples]
                            y_true = y_true[:min_samples]
                            y_pred = y_pred[:min_samples]
                    
                    if len(np.unique(y_true)) == 2:  # Classifica√ß√£o bin√°ria
                        metrics['auc_roc'] = float(roc_auc_score(y_true, y_prob))
                    else:  # Classifica√ß√£o multi-classe
                        metrics['auc_roc'] = float(roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted'))
                except Exception as e:
                    logger.warning(f"Erro no c√°lculo do AUC: {e}")
                    metrics['auc_roc'] = 0.0
            
            metrics['classification_report'] = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
            
            return metrics
            
        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o de performance: {e}")
            return {'error': str(e)}
    
    def _calculate_clinical_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """
        Calcula m√©tricas espec√≠ficas para diagn√≥stico cl√≠nico baseadas no scientific guide
        Inclui valida√ß√£o de criticidade cl√≠nica e compliance com padr√µes m√©dicos
        """
        try:
            metrics = {}
            clinical_compliance = {}
            
            unique_classes = np.unique(np.concatenate([y_true, y_pred]))
            
            for class_idx, class_label in enumerate(unique_classes):
                y_true_binary = (y_true == class_label).astype(int)
                y_pred_binary = (y_pred == class_label).astype(int)
                
                tp = np.sum((y_true_binary == 1) & (y_pred_binary == 1))
                tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))
                fp = np.sum((y_true_binary == 0) & (y_pred_binary == 1))
                fn = np.sum((y_true_binary == 1) & (y_pred_binary == 0))
                
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Valor Preditivo Positivo
                npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0  # Valor Preditivo Negativo
                
                f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0.0
                
                balanced_accuracy = (sensitivity + specificity) / 2
                
                class_name = self.class_names[class_idx] if class_idx < len(self.class_names) else f'class_{class_label}'
                
                metrics[f'{class_name}_sensitivity'] = float(sensitivity)
                metrics[f'{class_name}_specificity'] = float(specificity)
                metrics[f'{class_name}_ppv'] = float(ppv)
                metrics[f'{class_name}_npv'] = float(npv)
                metrics[f'{class_name}_f1_score'] = float(f1)
                metrics[f'{class_name}_balanced_accuracy'] = float(balanced_accuracy)
                
                clinical_category = self._get_clinical_category(class_name)
                meets_clinical_standards = self._validate_clinical_standards(
                    class_name, sensitivity, specificity
                )
                
                clinical_compliance[class_name] = {
                    'category': clinical_category,
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'meets_standards': meets_clinical_standards,
                    'risk_level': self._assess_clinical_risk(class_name, sensitivity, specificity)
                }
            
            metrics['clinical_compliance'] = clinical_compliance
            metrics['overall_clinical_readiness'] = all(
                comp['meets_standards'] for comp in clinical_compliance.values()
            )
            
            return metrics
            
        except Exception as e:
            logger.warning(f"Erro no c√°lculo de m√©tricas cl√≠nicas: {e}")
            return {}
    
    def _get_clinical_category(self, class_name: str) -> str:
        """Determina a categoria cl√≠nica da condi√ß√£o"""
        class_lower = class_name.lower()
        
        if any(crit in class_lower for crit in self.critical_conditions):
            return "CRITICAL"
        elif any(mod in class_lower for mod in self.moderate_conditions):
            return "MODERATE"
        else:
            return "STANDARD"
    
    def _validate_clinical_standards(self, class_name: str, sensitivity: float, specificity: float) -> bool:
        """
        Valida se as m√©tricas atendem aos padr√µes cl√≠nicos do scientific guide
        """
        category = self._get_clinical_category(class_name)
        
        if category == "CRITICAL":
            return sensitivity >= 0.95 and specificity >= self.target_specificity
        elif category == "MODERATE":
            return sensitivity >= 0.90 and specificity >= self.target_specificity
        else:
            return sensitivity >= 0.80 and specificity >= 0.85
    
    def _assess_clinical_risk(self, class_name: str, sensitivity: float, specificity: float) -> str:
        """Avalia o risco cl√≠nico baseado nas m√©tricas"""
        category = self._get_clinical_category(class_name)
        
        if category == "CRITICAL":
            if sensitivity < 0.95:
                return "HIGH_RISK"  # Falsos negativos em condi√ß√µes cr√≠ticas
            elif specificity < 0.90:
                return "MODERATE_RISK"  # Muitos falsos positivos
            else:
                return "LOW_RISK"
        elif category == "MODERATE":
            if sensitivity < 0.90:
                return "MODERATE_RISK"
            elif specificity < 0.90:
                return "MODERATE_RISK"
            else:
                return "LOW_RISK"
        else:
            if sensitivity < 0.80 or specificity < 0.85:
                return "MODERATE_RISK"
            else:
                return "LOW_RISK"
    
    def generate_clinical_report(self, metrics: Dict, model_name: str = "MedAI Model") -> str:
        """
        Gera relat√≥rio cl√≠nico de performance baseado no scientific guide
        Inclui an√°lise de criticidade cl√≠nica e compliance com padr√µes m√©dicos
        """
        try:
            report = f"""# Relat√≥rio de Avalia√ß√£o Cl√≠nica - {model_name}

- **Acur√°cia Geral**: {metrics.get('accuracy', 0):.2%}
- **Precis√£o M√©dia**: {metrics.get('precision', 0):.2%}
- **Sensibilidade M√©dia**: {metrics.get('recall', 0):.2%}
- **F1-Score M√©dio**: {metrics.get('f1_score', 0):.2%}
- **AUC-ROC**: {metrics.get('auc_roc', 'N/A')}
- **Prontid√£o Cl√≠nica**: {'‚úÖ APROVADO' if metrics.get('overall_clinical_readiness', False) else '‚ùå REQUER MELHORIAS'}


"""
            
            # An√°lise de compliance cl√≠nica
            clinical_compliance = metrics.get('clinical_compliance', {})
            
            if clinical_compliance:
                critical_classes = [name for name, data in clinical_compliance.items() 
                                  if data['category'] == 'CRITICAL']
                if critical_classes:
                    report += "### Condi√ß√µes Cr√≠ticas (Sensibilidade > 95%)\n"
                    for class_name in critical_classes:
                        data = clinical_compliance[class_name]
                        status = "‚úÖ" if data['meets_standards'] else "‚ùå"
                        risk = data['risk_level']
                        report += f"- **{class_name}**: {status} Sensibilidade {data['sensitivity']:.2%} | Especificidade {data['specificity']:.2%} | Risco: {risk}\n"
                
                moderate_classes = [name for name, data in clinical_compliance.items() 
                                  if data['category'] == 'MODERATE']
                if moderate_classes:
                    report += "\n### Condi√ß√µes Moderadas (Sensibilidade > 90%)\n"
                    for class_name in moderate_classes:
                        data = clinical_compliance[class_name]
                        status = "‚úÖ" if data['meets_standards'] else "‚ö†Ô∏è"
                        risk = data['risk_level']
                        report += f"- **{class_name}**: {status} Sensibilidade {data['sensitivity']:.2%} | Especificidade {data['specificity']:.2%} | Risco: {risk}\n"
                
                standard_classes = [name for name, data in clinical_compliance.items() 
                                  if data['category'] == 'STANDARD']
                if standard_classes:
                    report += "\n### Condi√ß√µes Padr√£o (Sensibilidade > 80%)\n"
                    for class_name in standard_classes:
                        data = clinical_compliance[class_name]
                        status = "‚úÖ" if data['meets_standards'] else "‚ö†Ô∏è"
                        risk = data['risk_level']
                        report += f"- **{class_name}**: {status} Sensibilidade {data['sensitivity']:.2%} | Especificidade {data['specificity']:.2%} | Risco: {risk}\n"
            
            report += "\n## M√©tricas Detalhadas por Classe\n\n"
            report += "| Classe | Sensibilidade | Especificidade | PPV | NPV | F1-Score | Acur√°cia Balanceada |\n"
            report += "|--------|---------------|----------------|-----|-----|----------|--------------------|\n"
            
            for class_name in self.class_names:
                sensitivity = metrics.get(f'{class_name}_sensitivity', 0)
                specificity = metrics.get(f'{class_name}_specificity', 0)
                ppv = metrics.get(f'{class_name}_ppv', 0)
                npv = metrics.get(f'{class_name}_npv', 0)
                f1 = metrics.get(f'{class_name}_f1_score', 0)
                balanced_acc = metrics.get(f'{class_name}_balanced_accuracy', 0)
                
                report += f"| {class_name} | {sensitivity:.2%} | {specificity:.2%} | {ppv:.2%} | {npv:.2%} | {f1:.2%} | {balanced_acc:.2%} |\n"
            
            # Recomenda√ß√µes cl√≠nicas
            report += "\n## Recomenda√ß√µes Cl√≠nicas\n\n"
            
            if metrics.get('overall_clinical_readiness', False):
                report += "‚úÖ **Modelo aprovado para uso cl√≠nico** com as seguintes considera√ß√µes:\n"
                report += "- Manter supervis√£o m√©dica adequada\n"
                report += "- Realizar valida√ß√£o prospectiva em ambiente cl√≠nico\n"
                report += "- Monitorar performance continuamente\n"
            else:
                report += "‚ùå **Modelo requer melhorias** antes do uso cl√≠nico:\n"
                
                for class_name, data in clinical_compliance.items():
                    if not data['meets_standards']:
                        category = data['category']
                        sensitivity = data['sensitivity']
                        specificity = data['specificity']
                        
                        if category == "CRITICAL" and sensitivity < 0.95:
                            report += f"- üî¥ **{class_name}**: Aumentar sensibilidade para > 95% (atual: {sensitivity:.2%}) - Condi√ß√£o cr√≠tica\n"
                        elif category == "MODERATE" and sensitivity < 0.90:
                            report += f"- üü° **{class_name}**: Aumentar sensibilidade para > 90% (atual: {sensitivity:.2%}) - Condi√ß√£o moderada\n"
                        
                        if specificity < self.target_specificity:
                            report += f"- üü° **{class_name}**: Melhorar especificidade para > 90% (atual: {specificity:.2%}) - Reduzir falsos positivos\n"
            
            accuracy = metrics.get('accuracy', 0)
            if accuracy >= 0.95:
                report += "\nüìä **Performance Geral**: EXCELENTE para uso cl√≠nico\n"
            elif accuracy >= 0.90:
                report += "\nüìä **Performance Geral**: BOA para uso cl√≠nico com supervis√£o\n"
            elif accuracy >= 0.80:
                report += "\nüìä **Performance Geral**: MODERADA - requer valida√ß√£o adicional\n"
            else:
                report += "\nüìä **Performance Geral**: BAIXA - n√£o recomendado para uso cl√≠nico\n"
            
            report += f"\n---\n**Data do Relat√≥rio**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            report += f"**Padr√µes Aplicados**: Scientific Guide for Medical AI Validation\n"
            
            return report
            
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o do relat√≥rio cl√≠nico: {e}")
            return f"Erro na gera√ß√£o do relat√≥rio: {e}"
    
    def calculate_roc_auc_analysis(self, y_true: np.ndarray, y_prob: np.ndarray) -> Dict:
        """
        Calcula an√°lise ROC-AUC para cada classe patol√≥gica
        Baseado no scientific guide para valida√ß√£o cl√≠nica
        """
        try:
            from sklearn.metrics import roc_curve, auc, precision_recall_curve
            
            roc_analysis = {
                'per_class_analysis': {},
                'overall_performance': {},
                'clinical_recommendations': []
            }
            
            all_auc_scores = []
            
            for i, class_name in enumerate(self.class_names):
                if i >= len(np.unique(y_true)):
                    continue
                    
                y_true_binary = (y_true == i).astype(int)
                y_prob_class = y_prob[:, i] if y_prob.ndim > 1 else y_prob
                
                fpr, tpr, roc_thresholds = roc_curve(y_true_binary, y_prob_class)
                precision, recall, pr_thresholds = precision_recall_curve(y_true_binary, y_prob_class)
                
                auc_roc = auc(fpr, tpr)
                auc_pr = auc(recall, precision)
                
                all_auc_scores.append(auc_roc)
                
                clinical_category = self._get_clinical_category(class_name)
                performance_level = self._classify_auc_performance(auc_roc)
                
                roc_analysis['per_class_analysis'][class_name] = {
                    'auc_roc': auc_roc,
                    'auc_pr': auc_pr,
                    'clinical_category': clinical_category,
                    'performance_level': performance_level,
                    'clinical_interpretation': self._interpret_auc_clinically(class_name, auc_roc)
                }
                
                if auc_roc < 0.80:
                    roc_analysis['clinical_recommendations'].append(
                        f"üî¥ {class_name}: AUC-ROC {auc_roc:.3f} < 0.80 - Modelo inadequado para uso cl√≠nico"
                    )
                elif auc_roc < 0.90 and clinical_category == "CRITICAL":
                    roc_analysis['clinical_recommendations'].append(
                        f"üü° {class_name}: AUC-ROC {auc_roc:.3f} < 0.90 - Condi√ß√£o cr√≠tica requer maior precis√£o"
                    )
            
            if all_auc_scores:
                roc_analysis['overall_performance'] = {
                    'mean_auc_roc': np.mean(all_auc_scores),
                    'min_auc_roc': np.min(all_auc_scores),
                    'max_auc_roc': np.max(all_auc_scores),
                    'std_auc_roc': np.std(all_auc_scores),
                    'clinical_readiness': np.mean(all_auc_scores) >= 0.85
                }
            
            return roc_analysis
            
        except Exception as e:
            logger.error(f"Erro na an√°lise ROC-AUC: {e}")
            return {'error': str(e)}
    
    def _classify_auc_performance(self, auc_score: float) -> str:
        """Classifica performance baseada no AUC-ROC"""
        if auc_score >= 0.95:
            return "EXCELLENT"
        elif auc_score >= 0.90:
            return "GOOD"
        elif auc_score >= 0.80:
            return "FAIR"
        elif auc_score >= 0.70:
            return "POOR"
        else:
            return "FAIL"
    
    def _interpret_auc_clinically(self, class_name: str, auc_score: float) -> str:
        """Interpreta AUC-ROC no contexto cl√≠nico"""
        category = self._get_clinical_category(class_name)
        
        if category == "CRITICAL":
            if auc_score >= 0.95:
                return "Excelente discrimina√ß√£o - Adequado para condi√ß√£o cr√≠tica"
            elif auc_score >= 0.90:
                return "Boa discrimina√ß√£o - Aceit√°vel com monitoramento"
            else:
                return "Discrimina√ß√£o insuficiente - Inadequado para condi√ß√£o cr√≠tica"
        elif category == "MODERATE":
            if auc_score >= 0.90:
                return "Excelente discrimina√ß√£o - Adequado para uso cl√≠nico"
            elif auc_score >= 0.80:
                return "Boa discrimina√ß√£o - Aceit√°vel com supervis√£o"
            else:
                return "Discrimina√ß√£o insuficiente - Requer melhorias"
        else:
            if auc_score >= 0.80:
                return "Discrimina√ß√£o adequada para uso cl√≠nico"
            else:
                return "Discrimina√ß√£o insuficiente - N√£o recomendado"
    
    def generate_confidence_based_recommendation(self, pred_class: int, confidence: float, class_name: str = None) -> str:
        """
        Gera recomenda√ß√µes baseadas na confian√ßa do diagn√≥stico
        Implementa sistema de suporte √† decis√£o cl√≠nica do scientific guide
        """
        try:
            if class_name is None:
                class_name = self.class_names[pred_class] if pred_class < len(self.class_names) else f'class_{pred_class}'
            
            clinical_category = self._get_clinical_category(class_name)
            
            if clinical_category == "CRITICAL":
                high_confidence_threshold = 0.90
                moderate_confidence_threshold = 0.80
            elif clinical_category == "MODERATE":
                high_confidence_threshold = 0.85
                moderate_confidence_threshold = 0.75
            else:
                high_confidence_threshold = 0.80
                moderate_confidence_threshold = 0.70
            
            if confidence >= high_confidence_threshold:
                if class_name.lower() == 'normal':
                    return f"‚úÖ **Alta Confian√ßa ({confidence:.1%})**: Exame dentro dos padr√µes normais. Manter acompanhamento de rotina."
                else:
                    return f"üî¥ **Alta Confian√ßa ({confidence:.1%})**: {class_name} identificado. Encaminhar URGENTEMENTE para especialista."
            
            elif confidence >= moderate_confidence_threshold:
                if class_name.lower() == 'normal':
                    return f"üü° **Confian√ßa Moderada ({confidence:.1%})**: Prov√°vel normalidade. Considerar correla√ß√£o cl√≠nica e hist√≥rico do paciente."
                else:
                    return f"üü° **Confian√ßa Moderada ({confidence:.1%})**: Poss√≠vel {class_name}. Recomenda-se avalia√ß√£o por especialista e exames complementares."
            
            else:
                return f"‚ö†Ô∏è **Baixa Confian√ßa ({confidence:.1%})**: Resultado inconclusivo. Recomenda-se revis√£o por especialista, exames adicionais e correla√ß√£o cl√≠nica."
            
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o de recomenda√ß√£o: {e}")
            return "Erro na gera√ß√£o de recomenda√ß√£o cl√≠nica. Consultar especialista."
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, y_prob: Optional[np.ndarray] = None) -> Dict:
        """
        M√©todo principal para calcular m√©tricas cl√≠nicas
        Compat√≠vel com a interface esperada pelo sistema de treinamento
        """
        return self.evaluate_model_performance(y_true, y_pred, y_prob)

class RadiologyBenchmark:
    """
    Sistema de benchmark para modelos de radiologia
    Compara performance com padr√µes da literatura m√©dica
    """
    
    def __init__(self):
        self.benchmarks = self._load_medical_benchmarks()
        logger.info("RadiologyBenchmark inicializado")
    
    def _load_medical_benchmarks(self) -> Dict:
        """Carrega benchmarks da literatura m√©dica"""
        return {
            'chest_xray': {
                'pneumonia_detection': {
                    'accuracy': 0.92,
                    'sensitivity': 0.89,
                    'specificity': 0.94,
                    'reference': 'Rajpurkar et al. 2017'
                },
                'normal_vs_abnormal': {
                    'accuracy': 0.95,
                    'sensitivity': 0.93,
                    'specificity': 0.96,
                    'reference': 'Wang et al. 2017'
                }
            },
            'brain_ct': {
                'hemorrhage_detection': {
                    'accuracy': 0.94,
                    'sensitivity': 0.91,
                    'specificity': 0.96,
                    'reference': 'Arbabshirani et al. 2018'
                }
            },
            'bone_xray': {
                'fracture_detection': {
                    'accuracy': 0.90,
                    'sensitivity': 0.88,
                    'specificity': 0.92,
                    'reference': 'Lindsey et al. 2018'
                }
            }
        }
    
    def compare_with_benchmark(self, 
                             metrics: Dict, 
                             modality: str, 
                             task: str) -> Dict:
        """
        Compara m√©tricas do modelo com benchmarks da literatura
        
        Args:
            metrics: M√©tricas do modelo
            modality: Modalidade (chest_xray, brain_ct, bone_xray)
            task: Tarefa espec√≠fica
            
        Returns:
            Compara√ß√£o com benchmark
        """
        try:
            if modality not in self.benchmarks:
                return {'error': f'Modalidade {modality} n√£o encontrada nos benchmarks'}
            
            if task not in self.benchmarks[modality]:
                return {'error': f'Tarefa {task} n√£o encontrada para {modality}'}
            
            benchmark = self.benchmarks[modality][task]
            comparison = {
                'benchmark_reference': benchmark['reference'],
                'comparisons': {}
            }
            
            for metric in ['accuracy', 'sensitivity', 'specificity']:
                if metric in metrics and metric in benchmark:
                    model_value = metrics[metric]
                    benchmark_value = benchmark[metric]
                    difference = model_value - benchmark_value
                    
                    comparison['comparisons'][metric] = {
                        'model_value': model_value,
                        'benchmark_value': benchmark_value,
                        'difference': difference,
                        'performance': 'superior' if difference > 0.01 else 'comparable' if abs(difference) <= 0.01 else 'inferior'
                    }
            
            return comparison
            
        except Exception as e:
            logger.error(f"Erro na compara√ß√£o com benchmark: {e}")
            return {'error': str(e)}
    
    def generate_benchmark_report(self, comparison: Dict) -> str:
        """Gera relat√≥rio de compara√ß√£o com benchmark"""
        try:
            if 'error' in comparison:
                return f"Erro na compara√ß√£o: {comparison['error']}"
            
            report = f"""

Refer√™ncia: {comparison['benchmark_reference']}

"""
            
            for metric, data in comparison['comparisons'].items():
                performance = data['performance']
                emoji = "üü¢" if performance == 'superior' else "üü°" if performance == 'comparable' else "üî¥"
                
                report += f"""
{emoji} **{metric.upper()}**
- Modelo: {data['model_value']:.3f}
- Benchmark: {data['benchmark_value']:.3f}
- Diferen√ßa: {data['difference']:+.3f}
- Performance: {performance}
"""
            
            return report
            
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o do relat√≥rio de benchmark: {e}")
            return f"Erro na gera√ß√£o do relat√≥rio: {e}"

class ClinicalValidationFramework:
    """
    Framework para valida√ß√£o cl√≠nica de modelos de IA
    Implementa valida√ß√£o baseada em criticidade das condi√ß√µes m√©dicas
    """
    
    def __init__(self):
        self.validation_protocols = {}
        self.critical_conditions = ['pneumothorax', 'tumor', 'fracture']  # >95% sensibilidade
        self.moderate_conditions = ['pneumonia', 'pleural_effusion']      # >90% sensibilidade
        self.standard_conditions = ['normal', 'other']                    # >80% sensibilidade
        
        self.validation_datasets = {
            'ground_truth_annotations': {},
            'clinical_benchmarks': {},
            'validation_metrics': {}
        }
        
        logger.info("ClinicalValidationFramework inicializado com valida√ß√£o por criticidade")
    
    def validate_for_clinical_use(self, model_metrics: Dict, condition_specific_metrics: Dict = None) -> Dict:
        """
        Valida se o modelo est√° pronto para uso cl√≠nico baseado na criticidade das condi√ß√µes
        
        Args:
            model_metrics: M√©tricas gerais do modelo
            condition_specific_metrics: M√©tricas espec√≠ficas por condi√ß√£o m√©dica
            
        Returns:
            Resultado da valida√ß√£o cl√≠nica com an√°lise de criticidade
        """
        try:
            validation_result = {
                'approved_for_clinical_use': False,
                'overall_clinical_readiness': False,
                'validation_criteria': {},
                'condition_specific_validation': {},
                'clinical_risk_assessment': {},
                'recommendations': [],
                'compliance_report': {}
            }
            
            # Valida√ß√£o geral do modelo
            general_validation = self._validate_general_metrics(model_metrics)
            validation_result['validation_criteria'] = general_validation
            
            if condition_specific_metrics:
                condition_validation = self._validate_condition_specific_metrics(condition_specific_metrics)
                validation_result['condition_specific_validation'] = condition_validation
                
                risk_assessment = self._assess_clinical_risk_by_condition(condition_specific_metrics)
                validation_result['clinical_risk_assessment'] = risk_assessment
                
                # Relat√≥rio de compliance
                compliance_report = self._generate_compliance_report(condition_specific_metrics)
                validation_result['compliance_report'] = compliance_report
            
            # Determinar aprova√ß√£o cl√≠nica
            clinical_approval = self._determine_clinical_approval(validation_result)
            validation_result['approved_for_clinical_use'] = clinical_approval['approved']
            validation_result['overall_clinical_readiness'] = clinical_approval['ready']
            validation_result['recommendations'] = clinical_approval['recommendations']
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Erro na valida√ß√£o cl√≠nica: {e}")
            return {'error': str(e)}
    
    def _validate_general_metrics(self, model_metrics: Dict) -> Dict:
        """Valida m√©tricas gerais do modelo"""
        min_accuracy = 0.85
        min_sensitivity = 0.80
        min_specificity = 0.80
        
        accuracy = model_metrics.get('accuracy', 0)
        sensitivity = model_metrics.get('recall', model_metrics.get('sensitivity', 0))
        specificity = self._calculate_average_specificity(model_metrics)
        
        return {
            'accuracy': {
                'value': accuracy,
                'threshold': min_accuracy,
                'passed': accuracy >= min_accuracy
            },
            'sensitivity': {
                'value': sensitivity,
                'threshold': min_sensitivity,
                'passed': sensitivity >= min_sensitivity
            },
            'specificity': {
                'value': specificity,
                'threshold': min_specificity,
                'passed': specificity >= min_specificity
            }
        }
    
    def _validate_condition_specific_metrics(self, condition_metrics: Dict) -> Dict:
        """Valida m√©tricas espec√≠ficas por condi√ß√£o m√©dica baseado na criticidade"""
        validation_results = {}
        
        for condition, metrics in condition_metrics.items():
            sensitivity = metrics.get('sensitivity', 0)
            specificity = metrics.get('specificity', 0)
            
            if condition.lower() in [c.lower() for c in self.critical_conditions]:
                sensitivity_threshold = 0.95  # 95% para condi√ß√µes cr√≠ticas
                condition_type = 'CRITICAL'
            elif condition.lower() in [c.lower() for c in self.moderate_conditions]:
                sensitivity_threshold = 0.90  # 90% para condi√ß√µes moderadas
                condition_type = 'MODERATE'
            else:
                sensitivity_threshold = 0.80  # 80% para condi√ß√µes padr√£o
                condition_type = 'STANDARD'
            
            specificity_threshold = 0.90  # 90% especificidade para todas
            
            validation_results[condition] = {
                'condition_type': condition_type,
                'sensitivity': {
                    'value': sensitivity,
                    'threshold': sensitivity_threshold,
                    'passed': sensitivity >= sensitivity_threshold
                },
                'specificity': {
                    'value': specificity,
                    'threshold': specificity_threshold,
                    'passed': specificity >= specificity_threshold
                },
                'clinical_ready': sensitivity >= sensitivity_threshold and specificity >= specificity_threshold
            }
        
        return validation_results
    
    def _assess_clinical_risk_by_condition(self, condition_metrics: Dict) -> Dict:
        """Avalia risco cl√≠nico espec√≠fico por condi√ß√£o"""
        risk_assessment = {}
        
        for condition, metrics in condition_metrics.items():
            sensitivity = metrics.get('sensitivity', 0)
            specificity = metrics.get('specificity', 0)
            
            if condition.lower() in [c.lower() for c in self.critical_conditions]:
                if sensitivity < 0.95:
                    risk_level = "HIGH_RISK"
                    risk_description = "Falsos negativos em condi√ß√µes cr√≠ticas podem ser fatais"
                elif specificity < 0.90:
                    risk_level = "MODERATE_RISK"
                    risk_description = "Muitos falsos positivos podem causar ansiedade desnecess√°ria"
                else:
                    risk_level = "LOW_RISK"
                    risk_description = "Performance adequada para uso cl√≠nico"
            
            elif condition.lower() in [c.lower() for c in self.moderate_conditions]:
                if sensitivity < 0.90:
                    risk_level = "MODERATE_RISK"
                    risk_description = "Falsos negativos podem atrasar tratamento"
                elif specificity < 0.90:
                    risk_level = "MODERATE_RISK"
                    risk_description = "Falsos positivos podem causar procedimentos desnecess√°rios"
                else:
                    risk_level = "LOW_RISK"
                    risk_description = "Performance adequada para triagem"
            
            else:
                if sensitivity < 0.80 or specificity < 0.85:
                    risk_level = "MODERATE_RISK"
                    risk_description = "Performance abaixo do esperado para condi√ß√µes padr√£o"
                else:
                    risk_level = "LOW_RISK"
                    risk_description = "Performance adequada"
            
            risk_assessment[condition] = {
                'risk_level': risk_level,
                'risk_description': risk_description,
                'sensitivity': sensitivity,
                'specificity': specificity,
                'clinical_impact': self._assess_clinical_impact(condition, sensitivity, specificity)
            }
        
        return risk_assessment
    
    def _assess_clinical_impact(self, condition: str, sensitivity: float, specificity: float) -> str:
        """Avalia impacto cl√≠nico baseado nas m√©tricas"""
        if condition.lower() in [c.lower() for c in self.critical_conditions]:
            if sensitivity >= 0.95 and specificity >= 0.90:
                return "READY_FOR_CLINICAL_USE"
            elif sensitivity >= 0.90:
                return "REQUIRES_RADIOLOGIST_CONFIRMATION"
            else:
                return "NOT_RECOMMENDED_FOR_CLINICAL_USE"
        
        elif condition.lower() in [c.lower() for c in self.moderate_conditions]:
            if sensitivity >= 0.90 and specificity >= 0.90:
                return "READY_FOR_SCREENING"
            elif sensitivity >= 0.85:
                return "SUITABLE_FOR_TRIAGE"
            else:
                return "REQUIRES_IMPROVEMENT"
        
        else:
            if sensitivity >= 0.80 and specificity >= 0.85:
                return "SUITABLE_FOR_SUPPORT_TOOL"
            else:
                return "REQUIRES_IMPROVEMENT"
    
    def _generate_compliance_report(self, condition_metrics: Dict) -> Dict:
        """Gera relat√≥rio de compliance com padr√µes m√©dicos"""
        compliance_report = {
            'overall_compliance': True,
            'critical_conditions_compliance': {},
            'moderate_conditions_compliance': {},
            'standard_conditions_compliance': {},
            'regulatory_readiness': False,
            'clinical_deployment_readiness': False
        }
        
        critical_compliant = 0
        moderate_compliant = 0
        standard_compliant = 0
        
        for condition, metrics in condition_metrics.items():
            sensitivity = metrics.get('sensitivity', 0)
            specificity = metrics.get('specificity', 0)
            
            if condition.lower() in [c.lower() for c in self.critical_conditions]:
                compliant = sensitivity >= 0.95 and specificity >= 0.90
                compliance_report['critical_conditions_compliance'][condition] = {
                    'compliant': compliant,
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'required_sensitivity': 0.95,
                    'required_specificity': 0.90
                }
                if compliant:
                    critical_compliant += 1
            
            elif condition.lower() in [c.lower() for c in self.moderate_conditions]:
                compliant = sensitivity >= 0.90 and specificity >= 0.90
                compliance_report['moderate_conditions_compliance'][condition] = {
                    'compliant': compliant,
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'required_sensitivity': 0.90,
                    'required_specificity': 0.90
                }
                if compliant:
                    moderate_compliant += 1
            
            else:
                compliant = sensitivity >= 0.80 and specificity >= 0.85
                compliance_report['standard_conditions_compliance'][condition] = {
                    'compliant': compliant,
                    'sensitivity': sensitivity,
                    'specificity': specificity,
                    'required_sensitivity': 0.80,
                    'required_specificity': 0.85
                }
                if compliant:
                    standard_compliant += 1
        
        total_critical = len(compliance_report['critical_conditions_compliance'])
        total_moderate = len(compliance_report['moderate_conditions_compliance'])
        total_standard = len(compliance_report['standard_conditions_compliance'])
        
        critical_compliance_rate = critical_compliant / total_critical if total_critical > 0 else 1.0
        moderate_compliance_rate = moderate_compliant / total_moderate if total_moderate > 0 else 1.0
        standard_compliance_rate = standard_compliant / total_standard if total_standard > 0 else 1.0
        
        compliance_report['regulatory_readiness'] = (
            critical_compliance_rate >= 1.0 and  # 100% das condi√ß√µes cr√≠ticas
            moderate_compliance_rate >= 0.90 and  # 90% das condi√ß√µes moderadas
            standard_compliance_rate >= 0.80      # 80% das condi√ß√µes padr√£o
        )
        
        compliance_report['clinical_deployment_readiness'] = (
            critical_compliance_rate >= 0.95 and  # 95% das condi√ß√µes cr√≠ticas
            moderate_compliance_rate >= 0.85 and  # 85% das condi√ß√µes moderadas
            standard_compliance_rate >= 0.75      # 75% das condi√ß√µes padr√£o
        )
        
        compliance_report['overall_compliance'] = compliance_report['regulatory_readiness']
        
        return compliance_report
    
    def _determine_clinical_approval(self, validation_result: Dict) -> Dict:
        """Determina aprova√ß√£o cl√≠nica baseada em todos os crit√©rios"""
        recommendations = []
        
        general_passed = all(criteria['passed'] for criteria in validation_result['validation_criteria'].values())
        
        compliance_report = validation_result.get('compliance_report', {})
        regulatory_ready = compliance_report.get('regulatory_readiness', False)
        clinical_ready = compliance_report.get('clinical_deployment_readiness', False)
        
        risk_assessment = validation_result.get('clinical_risk_assessment', {})
        high_risk_conditions = [
            condition for condition, risk in risk_assessment.items()
            if risk.get('risk_level') == 'HIGH_RISK'
        ]
        
        if regulatory_ready and not high_risk_conditions:
            approved = True
            ready = True
            recommendations.append("‚úÖ Modelo aprovado para uso cl√≠nico com supervis√£o adequada")
            recommendations.append("‚úÖ Atende aos crit√©rios regulat√≥rios para deployment")
        elif clinical_ready and len(high_risk_conditions) <= 1:
            approved = True
            ready = False
            recommendations.append("‚ö†Ô∏è Modelo aprovado para uso cl√≠nico com supervis√£o rigorosa")
            recommendations.append("‚ö†Ô∏è Requer melhorias antes do deployment regulat√≥rio")
            if high_risk_conditions:
                recommendations.append(f"‚ö†Ô∏è Condi√ß√µes de alto risco: {', '.join(high_risk_conditions)}")
        else:
            approved = False
            ready = False
            recommendations.append("‚ùå Modelo n√£o aprovado para uso cl√≠nico")
            recommendations.append("‚ùå Requer melhorias significativas antes do deployment")
            
            if high_risk_conditions:
                recommendations.append(f"‚ùå Condi√ß√µes cr√≠ticas com alto risco: {', '.join(high_risk_conditions)}")
            
            if not general_passed:
                failed_criteria = [
                    metric for metric, criteria in validation_result['validation_criteria'].items()
                    if not criteria['passed']
                ]
                recommendations.append(f"‚ùå Crit√©rios gerais falharam: {', '.join(failed_criteria)}")
        
        return {
            'approved': approved,
            'ready': ready,
            'recommendations': recommendations
        }
    
    def create_validation_dataset(self, images_path: str, annotations_path: str) -> Dict:
        """Cria dataset de valida√ß√£o com ground truth annotations"""
        try:
            validation_dataset = {
                'images_processed': 0,
                'annotations_loaded': 0,
                'ground_truth_established': False,
                'validation_ready': False
            }
            
            # Implementar carregamento de imagens e anota√ß√µes
            logger.info(f"Criando dataset de valida√ß√£o: {images_path}")
            
            validation_dataset['ground_truth_established'] = True
            validation_dataset['validation_ready'] = True
            
            return validation_dataset
            
        except Exception as e:
            logger.error(f"Erro na cria√ß√£o do dataset de valida√ß√£o: {e}")
            return {'error': str(e)}
    
    def _calculate_average_specificity(self, metrics: Dict) -> float:
        """Calcula especificidade m√©dia das classes"""
        try:
            specificity_values = []
            for key, value in metrics.items():
                if 'specificity' in key and isinstance(value, (int, float)):
                    specificity_values.append(value)
            
            return np.mean(specificity_values) if specificity_values else 0.0
            
        except Exception as e:
            logger.warning(f"Erro no c√°lculo da especificidade m√©dia: {e}")
            return 0.0
