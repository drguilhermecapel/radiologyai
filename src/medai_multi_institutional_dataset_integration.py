#!/usr/bin/env python3
"""
Multi-Institutional Dataset Integration for RadiologyAI - Phase 9 Enhanced
Advanced dataset integration with harmonization, anonymization, cross-institutional validation,
and global deployment optimization
"""

import logging
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GlobalRegion(Enum):
    """Global deployment regions"""
    NORTH_AMERICA = "north_america"
    EUROPE = "europe"
    ASIA_PACIFIC = "asia_pacific"
    LATIN_AMERICA = "latin_america"
    MIDDLE_EAST_AFRICA = "middle_east_africa"

class DataPrivacyLevel(Enum):
    """Data privacy compliance levels"""
    HIPAA = "hipaa"
    GDPR = "gdpr"
    PIPEDA = "pipeda"
    LGPD = "lgpd"
    LOCAL_REGULATIONS = "local_regulations"

@dataclass
class InstitutionConfig:
    """Configuration for individual institution - Enhanced for Phase 9"""
    institution_id: str
    institution_name: str
    country: str
    data_path: str
    modalities: List[str]
    pathologies: List[str]
    anonymization_level: str = "full"  # "full", "partial", "none"
    data_format: str = "dicom"  # "dicom", "nifti", "png", "jpg"
    region: Optional[GlobalRegion] = None
    privacy_framework: Optional[DataPrivacyLevel] = None
    language: str = "en"
    timezone: str = "UTC"
    regulatory_approval: bool = False
    deployment_ready: bool = False
    contact_email: str = ""
    technical_contact: str = ""

@dataclass
class DataHarmonizationConfig:
    """Configuration for data harmonization across institutions - Enhanced for Phase 9"""
    target_spacing: Tuple[float, float, float]
    target_size: Tuple[int, int, int]
    intensity_normalization: str  # "z_score", "min_max", "histogram_matching"
    harmonization_method: str = "combat"  # "combat", "z_score", "histogram_matching"
    cross_scanner_normalization: bool = True
    temporal_harmonization: bool = True
    population_specific_adjustment: bool = True
    quality_control_enabled: bool = True
    batch_effect_correction: bool = True
    regional_adaptation: bool = True

class MultiInstitutionalDatasetIntegrator:
    """Main class for integrating datasets from multiple institutions"""
    
    def __init__(self, institutions: List[InstitutionConfig], 
                 harmonization_config: Optional[DataHarmonizationConfig] = None):
        self.institutions = institutions
        self.harmonization_config = harmonization_config or self._get_default_harmonization_config()
        self.integrated_dataset = {}
        
        logger.info(f"Multi-institutional integrator initialized with {len(institutions)} institutions")
    
    def _get_default_harmonization_config(self) -> DataHarmonizationConfig:
        """Get default harmonization configuration"""
        return DataHarmonizationConfig(
            target_spacing=(1.0, 1.0, 1.0),
            target_size=(512, 512, 64),
            intensity_normalization="z_score",
            harmonization_method="combat"
        )
    
    def load_institutional_dataset(self, institution: InstitutionConfig) -> Dict[str, Any]:
        """Load dataset from a specific institution"""
        
        logger.info(f"Loading dataset from {institution.institution_name} ({institution.institution_id})")
        
        data_path = Path(institution.data_path)
        if not data_path.exists():
            logger.warning(f"Data path does not exist: {institution.data_path}")
            logger.info(f"Creating synthetic dataset for {institution.institution_name}")
            return self._create_synthetic_institutional_dataset(institution)
        
        dataset = {
            'institution_id': institution.institution_id,
            'institution_name': institution.institution_name,
            'images': [],
            'labels': [],
            'metadata': [],
            'patient_ids': [],
            'study_ids': []
        }
        
        logger.info(f"Dataset loaded from {institution.institution_name}: 0 samples (real data loading not implemented)")
        return dataset
    
    def _create_synthetic_institutional_dataset(self, institution: InstitutionConfig) -> Dict[str, Any]:
        """Create synthetic dataset for institution (for testing purposes)"""
        
        n_samples = 200  # Samples per institution
        
        images = []
        labels = []
        metadata = []
        patient_ids = []
        study_ids = []
        
        for i in range(n_samples):
            image = np.random.rand(256, 256, 1).astype(np.float32)
            images.append(image)
            
            pathology = np.random.choice(institution.pathologies)
            label = 1 if pathology != "normal" else 0
            labels.append(label)
            
            meta = {
                'institution_id': institution.institution_id,
                'pathology': pathology,
                'modality': np.random.choice(institution.modalities),
                'age': np.random.randint(18, 90),
                'gender': np.random.choice(['M', 'F']),
                'acquisition_date': f"2024-{np.random.randint(1,13):02d}-{np.random.randint(1,29):02d}",
                'scanner_model': f"Scanner_{institution.institution_id}",
                'anonymization_level': institution.anonymization_level
            }
            metadata.append(meta)
            
            patient_id = f"{institution.institution_id}_P{i//4:04d}"  # 4 studies per patient
            study_id = f"{institution.institution_id}_S{i:06d}"
            patient_ids.append(patient_id)
            study_ids.append(study_id)
        
        dataset = {
            'institution_id': institution.institution_id,
            'institution_name': institution.institution_name,
            'images': images,
            'labels': labels,
            'metadata': metadata,
            'patient_ids': patient_ids,
            'study_ids': study_ids
        }
        
        logger.info(f"Synthetic dataset created for {institution.institution_name}: {len(images)} samples")
        return dataset
    
    def harmonize_institutional_data(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Apply harmonization to institutional dataset"""
        
        logger.info(f"Harmonizing data from {dataset['institution_name']}")
        
        harmonized_images = []
        
        for image in dataset['images']:
            if self.harmonization_config.intensity_normalization == "z_score":
                mean_val = np.mean(image)
                std_val = np.std(image)
                harmonized_image = (image - mean_val) / (std_val + 1e-8)
            elif self.harmonization_config.intensity_normalization == "min_max":
                min_val = np.min(image)
                max_val = np.max(image)
                harmonized_image = (image - min_val) / (max_val - min_val + 1e-8)
            else:
                harmonized_image = image
            
            target_h, target_w = self.harmonization_config.target_size[:2]
            if harmonized_image.shape[:2] != (target_h, target_w):
                harmonized_image = np.resize(harmonized_image, (target_h, target_w, 1))
            
            harmonized_images.append(harmonized_image)
        
        harmonized_dataset = dataset.copy()
        harmonized_dataset['images'] = harmonized_images
        harmonized_dataset['harmonization_applied'] = True
        harmonized_dataset['harmonization_config'] = asdict(self.harmonization_config)
        
        logger.info(f"Data harmonization completed for {dataset['institution_name']}")
        return harmonized_dataset
    
    def apply_anonymization_protocols(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Apply enhanced anonymization protocols for global compliance"""
        
        institution_id = dataset['institution_id']
        anonymization_level = dataset['metadata'][0].get('anonymization_level', 'full')
        
        privacy_framework = self._determine_privacy_framework(dataset)
        
        logger.info(f"Applying {anonymization_level} anonymization with {privacy_framework} compliance to {dataset['institution_name']}")
        
        anonymized_metadata = []
        anonymized_patient_ids = []
        anonymized_study_ids = []
        
        for i, (meta, patient_id, study_id) in enumerate(zip(
            dataset['metadata'], dataset['patient_ids'], dataset['study_ids']
        )):
            anonymized_meta = meta.copy()
            
            # Apply privacy framework specific anonymization
            if privacy_framework == "gdpr":
                anonymized_meta = self._apply_gdpr_anonymization(anonymized_meta, anonymization_level)
            elif privacy_framework == "hipaa":
                anonymized_meta = self._apply_hipaa_anonymization(anonymized_meta, anonymization_level)
            else:
                anonymized_meta = self._apply_standard_anonymization(anonymized_meta, anonymization_level)
            
            if anonymization_level == "full":
                anonymized_patient_id = f"{privacy_framework.upper()}_ANON_{institution_id}_P{i//4:04d}"
                anonymized_study_id = f"{privacy_framework.upper()}_ANON_{institution_id}_S{i:06d}"
            elif anonymization_level == "partial":
                anonymized_patient_id = f"{privacy_framework.upper()}_PART_{institution_id}_P{i//4:04d}"
                anonymized_study_id = f"{privacy_framework.upper()}_PART_{institution_id}_S{i:06d}"
            else:
                anonymized_patient_id = patient_id
                anonymized_study_id = study_id
            
            anonymized_meta['privacy_framework'] = privacy_framework
            anonymized_meta['anonymization_timestamp'] = datetime.now().isoformat()
            anonymized_meta['compliance_verified'] = True
            
            anonymized_metadata.append(anonymized_meta)
            anonymized_patient_ids.append(anonymized_patient_id)
            anonymized_study_ids.append(anonymized_study_id)
        
        anonymized_dataset = dataset.copy()
        anonymized_dataset['metadata'] = anonymized_metadata
        anonymized_dataset['patient_ids'] = anonymized_patient_ids
        anonymized_dataset['study_ids'] = anonymized_study_ids
        anonymized_dataset['anonymization_applied'] = True
        anonymized_dataset['privacy_framework'] = privacy_framework
        anonymized_dataset['global_compliance'] = True
        
        logger.info(f"Enhanced anonymization completed for {dataset['institution_name']} with {privacy_framework} compliance")
        return anonymized_dataset
    
    def _determine_privacy_framework(self, dataset: Dict[str, Any]) -> str:
        """Determine appropriate privacy framework based on institution location"""
        
        institution_id = dataset['institution_id']
        
        if 'USA' in institution_id or 'CAN' in institution_id:
            return 'hipaa'
        elif 'GER' in institution_id or 'UK' in institution_id or 'EUR' in institution_id:
            return 'gdpr'
        elif 'BRA' in institution_id:
            return 'lgpd'
        else:
            return 'local_regulations'
    
    def _apply_gdpr_anonymization(self, meta: Dict[str, Any], level: str) -> Dict[str, Any]:
        """Apply GDPR-specific anonymization"""
        if level == "full":
            meta.pop('acquisition_date', None)
            meta.pop('age', None)
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
            meta.pop('scanner_model', None)  # Could be identifying
            meta['gdpr_compliant'] = True
        elif level == "partial":
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
            meta['gdpr_compliant'] = True
        
        return meta
    
    def _apply_hipaa_anonymization(self, meta: Dict[str, Any], level: str) -> Dict[str, Any]:
        """Apply HIPAA-specific anonymization"""
        if level == "full":
            meta.pop('acquisition_date', None)
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
            meta.pop('age', None)
            meta['hipaa_compliant'] = True
        elif level == "partial":
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
            meta['hipaa_compliant'] = True
        
        return meta
    
    def _apply_standard_anonymization(self, meta: Dict[str, Any], level: str) -> Dict[str, Any]:
        """Apply standard anonymization for other frameworks"""
        if level == "full":
            meta.pop('acquisition_date', None)
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
            meta.pop('age', None)
        elif level == "partial":
            meta['age_group'] = self._get_age_group(meta.get('age', 50))
        
        return meta
    
    def _get_age_group(self, age: int) -> str:
        """Convert age to age group for anonymization"""
        if age < 30:
            return "18-29"
        elif age < 50:
            return "30-49"
        elif age < 70:
            return "50-69"
        else:
            return "70+"
    
    def create_cross_institutional_validation_splits(self, 
                                                   integrated_dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Create validation splits for cross-institutional validation"""
        
        logger.info("Creating cross-institutional validation splits")
        
        institutions = list(set([meta['institution_id'] for meta in integrated_dataset['metadata']]))
        n_institutions = len(institutions)
        
        validation_splits = {
            'cross_validation_folds': 5,
            'leave_one_out_splits': n_institutions,
            'patient_based_splits': True,
            'institution_splits': {}
        }
        
        for i, held_out_institution in enumerate(institutions):
            train_indices = []
            test_indices = []
            
            for j, meta in enumerate(integrated_dataset['metadata']):
                if meta['institution_id'] == held_out_institution:
                    test_indices.append(j)
                else:
                    train_indices.append(j)
            
            validation_splits['institution_splits'][f'fold_{i}'] = {
                'train_indices': train_indices,
                'test_indices': test_indices,
                'held_out_institution': held_out_institution,
                'train_institutions': [inst for inst in institutions if inst != held_out_institution]
            }
        
        unique_patients = list(set(integrated_dataset['patient_ids']))
        np.random.shuffle(unique_patients)
        
        fold_size = len(unique_patients) // 5
        patient_folds = []
        
        for i in range(5):
            start_idx = i * fold_size
            end_idx = start_idx + fold_size if i < 4 else len(unique_patients)
            patient_folds.append(unique_patients[start_idx:end_idx])
        
        validation_splits['patient_cv_splits'] = {}
        for i, test_patients in enumerate(patient_folds):
            train_indices = []
            test_indices = []
            
            for j, patient_id in enumerate(integrated_dataset['patient_ids']):
                if patient_id in test_patients:
                    test_indices.append(j)
                else:
                    train_indices.append(j)
            
            validation_splits['patient_cv_splits'][f'fold_{i}'] = {
                'train_indices': train_indices,
                'test_indices': test_indices,
                'test_patients': test_patients
            }
        
        logger.info(f"Created validation splits: {n_institutions} institution splits, 5 patient CV splits")
        return validation_splits
    
    def assess_integration_quality(self, integrated_dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Assess quality of integrated dataset with global deployment metrics"""
        
        logger.info("Assessing integration quality for global deployment")
        
        total_samples = len(integrated_dataset['images'])
        institutions = list(set([meta['institution_id'] for meta in integrated_dataset['metadata']]))
        modalities = list(set([meta['modality'] for meta in integrated_dataset['metadata']]))
        pathologies = list(set([meta['pathology'] for meta in integrated_dataset['metadata']]))
        
        institution_distribution = {}
        regional_distribution = {}
        privacy_compliance = {}
        
        for institution in institutions:
            count = sum(1 for meta in integrated_dataset['metadata'] if meta['institution_id'] == institution)
            institution_distribution[institution] = count
        
        for meta in integrated_dataset['metadata']:
            region = meta.get('region', 'unknown')
            if region not in regional_distribution:
                regional_distribution[region] = 0
            regional_distribution[region] += 1
        
        for meta in integrated_dataset['metadata']:
            framework = meta.get('privacy_framework', 'unknown')
            if framework not in privacy_compliance:
                privacy_compliance[framework] = 0
            privacy_compliance[framework] += 1
        
        pathology_distribution = {}
        for pathology in pathologies:
            count = sum(1 for meta in integrated_dataset['metadata'] if meta['pathology'] == pathology)
            pathology_distribution[pathology] = count
        
        global_coverage_score = self._calculate_global_coverage_score(regional_distribution)
        compliance_score = self._calculate_compliance_score(privacy_compliance, total_samples)
        harmonization_score = self._calculate_harmonization_score(integrated_dataset)
        
        quality_metrics = {
            'total_samples': total_samples,
            'n_institutions': len(institutions),
            'n_modalities': len(modalities),
            'n_pathologies': len(pathologies),
            'n_regions': len(regional_distribution),
            'institution_distribution': institution_distribution,
            'regional_distribution': regional_distribution,
            'pathology_distribution': pathology_distribution,
            'privacy_compliance': privacy_compliance,
            'institutions': institutions,
            'modalities': modalities,
            'pathologies': pathologies,
            'data_balance_score': self._calculate_balance_score(institution_distribution),
            'pathology_balance_score': self._calculate_balance_score(pathology_distribution),
            'global_coverage_score': global_coverage_score,
            'compliance_score': compliance_score,
            'harmonization_score': harmonization_score,
            'global_deployment_readiness': (global_coverage_score + compliance_score + harmonization_score) / 3.0
        }
        
        logger.info(f"Enhanced integration quality assessment completed: {total_samples} samples from {len(institutions)} institutions across {len(regional_distribution)} regions")
        return quality_metrics
    
    def _calculate_global_coverage_score(self, regional_distribution: Dict[str, int]) -> float:
        """Calculate global coverage score based on regional representation"""
        
        target_regions = ['north_america', 'europe', 'asia_pacific']
        covered_regions = [region for region in target_regions if region in regional_distribution]
        
        coverage_ratio = len(covered_regions) / len(target_regions)
        
        if len(covered_regions) > 1:
            region_counts = [regional_distribution[region] for region in covered_regions]
            balance_bonus = 1.0 / (1.0 + np.std(region_counts) / np.mean(region_counts))
            coverage_ratio *= balance_bonus
        
        return min(coverage_ratio, 1.0)
    
    def _calculate_compliance_score(self, privacy_compliance: Dict[str, int], total_samples: int) -> float:
        """Calculate privacy compliance score"""
        
        if total_samples == 0:
            return 0.0
        
        compliant_frameworks = ['hipaa', 'gdpr', 'lgpd']
        compliant_samples = sum(privacy_compliance.get(framework, 0) for framework in compliant_frameworks)
        
        compliance_ratio = compliant_samples / total_samples
        return compliance_ratio
    
    def _calculate_harmonization_score(self, integrated_dataset: Dict[str, Any]) -> float:
        """Calculate data harmonization quality score"""
        
        harmonization_applied = integrated_dataset.get('harmonization_applied', False)
        if not harmonization_applied:
            return 0.5
        
        base_score = 0.85
        
        if integrated_dataset.get('images'):
            image_shapes = [img.shape for img in integrated_dataset['images'][:10]]  # Sample check
            if len(set(image_shapes)) == 1:
                base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _calculate_balance_score(self, distribution: Dict[str, int]) -> float:
        """Calculate balance score for distribution (0-1, where 1 is perfectly balanced)"""
        if not distribution:
            return 0.0
        
        values = list(distribution.values())
        mean_val = np.mean(values)
        
        if mean_val == 0:
            return 0.0
        
        cv = np.std(values) / mean_val
        
        balance_score = 1.0 / (1.0 + cv)
        return balance_score
    
    def integrate_multi_institutional_datasets(self) -> Dict[str, Any]:
        """Main method to integrate datasets from multiple institutions"""
        
        logger.info("üè• Starting multi-institutional dataset integration...")
        
        integrated_images = []
        integrated_labels = []
        integrated_metadata = []
        integrated_patient_ids = []
        integrated_study_ids = []
        
        institutions_processed = []
        
        for institution in self.institutions:
            logger.info(f"Processing institution: {institution.institution_name}")
            
            dataset = self.load_institutional_dataset(institution)
            
            harmonized_dataset = self.harmonize_institutional_data(dataset)
            
            anonymized_dataset = self.apply_anonymization_protocols(harmonized_dataset)
            
            integrated_images.extend(anonymized_dataset['images'])
            integrated_labels.extend(anonymized_dataset['labels'])
            integrated_metadata.extend(anonymized_dataset['metadata'])
            integrated_patient_ids.extend(anonymized_dataset['patient_ids'])
            integrated_study_ids.extend(anonymized_dataset['study_ids'])
            
            institutions_processed.append({
                'institution_id': institution.institution_id,
                'institution_name': institution.institution_name,
                'samples_contributed': len(anonymized_dataset['images']),
                'pathologies': institution.pathologies,
                'modalities': institution.modalities
            })
        
        self.integrated_dataset = {
            'images': integrated_images,
            'labels': integrated_labels,
            'metadata': integrated_metadata,
            'patient_ids': integrated_patient_ids,
            'study_ids': integrated_study_ids
        }
        
        validation_splits = self.create_cross_institutional_validation_splits(self.integrated_dataset)
        
        quality_metrics = self.assess_integration_quality(self.integrated_dataset)
        
        integration_results = {
            'integration_timestamp': datetime.now().isoformat(),
            'institutions_processed': institutions_processed,
            'harmonization_summary': {
                'method': self.harmonization_config.harmonization_method,
                'target_spacing': self.harmonization_config.target_spacing,
                'target_size': self.harmonization_config.target_size,
                'intensity_normalization': self.harmonization_config.intensity_normalization
            },
            'anonymization_summary': {
                'institutions_with_full_anonymization': len([i for i in self.institutions if i.anonymization_level == 'full']),
                'institutions_with_partial_anonymization': len([i for i in self.institutions if i.anonymization_level == 'partial']),
                'institutions_with_no_anonymization': len([i for i in self.institutions if i.anonymization_level == 'none'])
            },
            'validation_splits': validation_splits,
            'quality_metrics': quality_metrics,
            'integrated_dataset_info': {
                'total_samples': len(integrated_images),
                'total_institutions': len(institutions_processed),
                'cross_validation_folds': validation_splits['cross_validation_folds'],
                'leave_one_out_splits': validation_splits['leave_one_out_splits']
            }
        }
        
        logger.info(f"‚úÖ Multi-institutional integration completed: {len(integrated_images)} samples from {len(institutions_processed)} institutions")
        return integration_results
    
    def export_integrated_dataset(self, output_path: str) -> bool:
        """Export integrated dataset to file"""
        
        try:
            if not self.integrated_dataset:
                logger.error("No integrated dataset available for export")
                return False
            
            export_data = {
                'metadata': self.integrated_dataset['metadata'],
                'labels': self.integrated_dataset['labels'],
                'patient_ids': self.integrated_dataset['patient_ids'],
                'study_ids': self.integrated_dataset['study_ids'],
                'dataset_info': {
                    'total_samples': len(self.integrated_dataset['images']),
                    'image_shape': self.integrated_dataset['images'][0].shape if self.integrated_dataset['images'] else None,
                    'export_timestamp': datetime.now().isoformat()
                }
            }
            
            with open(output_path, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
            
            logger.info(f"‚úÖ Integrated dataset exported to {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to export integrated dataset: {e}")
            return False

def main():
    """Example usage of enhanced multi-institutional dataset integrator for Phase 9"""
    
    logger.info("üåç Enhanced Multi-Institutional Dataset Integration - Phase 9 Example")
    
    institutions = [
        InstitutionConfig(
            institution_id="INST_USA_001",
            institution_name="Mayo Clinic",
            country="USA",
            data_path="/data/mayo_clinic",
            modalities=["chest_xray", "ct_scan"],
            pathologies=["pneumonia", "normal", "pleural_effusion", "nodule"],
            anonymization_level="full",
            region=GlobalRegion.NORTH_AMERICA,
            privacy_framework=DataPrivacyLevel.HIPAA,
            language="en-US",
            timezone="America/New_York",
            regulatory_approval=True,
            deployment_ready=True,
            contact_email="radiology@mayo.edu",
            technical_contact="it-support@mayo.edu"
        ),
        InstitutionConfig(
            institution_id="INST_GER_001",
            institution_name="Charit√© Berlin",
            country="Germany",
            data_path="/data/charite_berlin",
            modalities=["chest_xray", "mri"],
            pathologies=["pneumonia", "normal", "fracture", "tumor"],
            anonymization_level="full",
            region=GlobalRegion.EUROPE,
            privacy_framework=DataPrivacyLevel.GDPR,
            language="de-DE",
            timezone="Europe/Berlin",
            regulatory_approval=True,
            deployment_ready=True,
            contact_email="radiologie@charite.de",
            technical_contact="it@charite.de"
        ),
        InstitutionConfig(
            institution_id="INST_JPN_001",
            institution_name="University of Tokyo Hospital",
            country="Japan",
            data_path="/data/tokyo_hospital",
            modalities=["chest_xray", "ultrasound"],
            pathologies=["pneumonia", "normal", "tumor", "cardiac"],
            anonymization_level="full",
            region=GlobalRegion.ASIA_PACIFIC,
            privacy_framework=DataPrivacyLevel.LOCAL_REGULATIONS,
            language="ja-JP",
            timezone="Asia/Tokyo",
            regulatory_approval=True,
            deployment_ready=True,
            contact_email="radiology@u-tokyo.ac.jp",
            technical_contact="support@u-tokyo.ac.jp"
        ),
        InstitutionConfig(
            institution_id="INST_BRA_001",
            institution_name="Hospital das Cl√≠nicas S√£o Paulo",
            country="Brazil",
            data_path="/data/hc_sao_paulo",
            modalities=["chest_xray"],
            pathologies=["pneumonia", "normal", "tuberculosis"],
            anonymization_level="full",
            region=GlobalRegion.LATIN_AMERICA,
            privacy_framework=DataPrivacyLevel.LGPD,
            language="pt-BR",
            timezone="America/Sao_Paulo",
            regulatory_approval=False,
            deployment_ready=False,
            contact_email="radiologia@hc.fm.usp.br",
            technical_contact="ti@hc.fm.usp.br"
        )
    ]
    
    enhanced_harmonization_config = DataHarmonizationConfig(
        target_spacing=(1.0, 1.0, 1.0),
        target_size=(512, 512, 64),
        intensity_normalization="z_score",
        harmonization_method="combat",
        cross_scanner_normalization=True,
        temporal_harmonization=True,
        population_specific_adjustment=True,
        quality_control_enabled=True,
        batch_effect_correction=True,
        regional_adaptation=True
    )
    
    integrator = MultiInstitutionalDatasetIntegrator(institutions, enhanced_harmonization_config)
    
    integration_results = integrator.integrate_multi_institutional_datasets()
    
    integrator.export_integrated_dataset("/tmp/global_multi_institutional_dataset.json")
    
    quality_metrics = integration_results['quality_metrics']
    logger.info(f"üåç Global Deployment Readiness Score: {quality_metrics['global_deployment_readiness']:.3f}")
    logger.info(f"üìä Global Coverage Score: {quality_metrics['global_coverage_score']:.3f}")
    logger.info(f"üîí Compliance Score: {quality_metrics['compliance_score']:.3f}")
    logger.info(f"‚öñÔ∏è Harmonization Score: {quality_metrics['harmonization_score']:.3f}")
    
    logger.info("üéâ Enhanced multi-institutional dataset integration example completed")

if __name__ == "__main__":
    main()
