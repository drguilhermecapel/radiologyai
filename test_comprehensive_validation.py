"""
Comprehensive validation testing for MedAI Radiologia production deployment
Tests all components including AI models, web interface, clinical monitoring, and deployment readiness
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import json
import time
import requests
import subprocess
from datetime import datetime
import numpy as np
import tensorflow as tf
from medai_integration_manager import MedAIIntegrationManager
from medai_clinical_monitoring_dashboard import ClinicalMonitoringDashboard
from medai_advanced_clinical_validation import AdvancedClinicalValidationFramework

def test_comprehensive_validation():
    """Execute comprehensive validation testing for production deployment"""
    print("ðŸš€ COMPREHENSIVE VALIDATION TESTING - MedAI Radiologia v4.0.0")
    print("=" * 80)
    
    validation_results = {
        'ai_models': {},
        'web_interface': {},
        'clinical_monitoring': {},
        'deployment_config': {},
        'performance_metrics': {},
        'security_compliance': {},
        'overall_status': 'PENDING'
    }
    
    try:
        print("\nðŸ§  Testing AI Models and Inference System...")
        try:
            test_ai_models_validation()
            ai_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… AI Models validation PASSED")
        except AssertionError as e:
            ai_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ AI Models validation FAILED: {e}")
        validation_results['ai_models'] = ai_validation
        
        print("\nðŸ“Š Testing Clinical Monitoring Dashboard...")
        try:
            test_clinical_monitoring_validation()
            monitoring_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Clinical Monitoring validation PASSED")
        except AssertionError as e:
            monitoring_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Clinical Monitoring validation FAILED: {e}")
        validation_results['clinical_monitoring'] = monitoring_validation
        
        print("\nðŸ¥ Testing Advanced Clinical Validation Framework...")
        try:
            test_advanced_clinical_framework()
            clinical_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Advanced Clinical Validation PASSED")
        except AssertionError as e:
            clinical_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Advanced Clinical Validation FAILED: {e}")
        validation_results['clinical_validation'] = clinical_validation
        
        print("\nâš™ï¸ Testing Production Configuration...")
        try:
            test_production_configuration()
            config_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Production Configuration validation PASSED")
        except AssertionError as e:
            config_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Production Configuration validation FAILED: {e}")
        validation_results['deployment_config'] = config_validation
        
        print("\nðŸ³ Testing Docker Configuration...")
        try:
            test_docker_configuration()
            docker_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Docker Configuration validation PASSED")
        except AssertionError as e:
            docker_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Docker Configuration validation FAILED: {e}")
        validation_results['docker_config'] = docker_validation
        
        print("\nâš¡ Testing Performance Optimization Methods...")
        try:
            test_performance_optimization()
            performance_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Performance Optimization validation PASSED")
        except AssertionError as e:
            performance_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Performance Optimization validation FAILED: {e}")
        validation_results['performance_metrics'] = performance_validation
        
        print("\nðŸ”’ Testing Security and Compliance...")
        try:
            test_security_compliance()
            security_validation = {'status': 'PASS', 'metrics': {'test_passed': True}}
            print("  âœ… Security and Compliance validation PASSED")
        except AssertionError as e:
            security_validation = {'status': 'FAIL', 'error': str(e)}
            print(f"  âŒ Security and Compliance validation FAILED: {e}")
        validation_results['security_compliance'] = security_validation
        
        passed_tests = sum(1 for result in validation_results.values() 
                          if isinstance(result, dict) and result.get('status') == 'PASS')
        total_tests = len([k for k, v in validation_results.items() 
                          if isinstance(v, dict) and 'status' in v])
        
        if passed_tests == total_tests:
            validation_results['overall_status'] = 'PASS'
            overall_status = "âœ… ALL TESTS PASSED"
        elif passed_tests >= total_tests * 0.8:
            validation_results['overall_status'] = 'PARTIAL_PASS'
            overall_status = "âš ï¸ PARTIAL PASS (80%+ tests passed)"
        else:
            validation_results['overall_status'] = 'FAIL'
            overall_status = "âŒ VALIDATION FAILED"
        
        print("\n" + "=" * 80)
        print("ðŸ“‹ COMPREHENSIVE VALIDATION REPORT")
        print("=" * 80)
        print(f"ðŸŽ¯ Overall Status: {overall_status}")
        print(f"ðŸ“Š Tests Passed: {passed_tests}/{total_tests}")
        print(f"ðŸ“… Validation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ðŸ¥ System Version: MedAI Radiologia v4.0.0")
        
        print(f"\nðŸ“ˆ DETAILED RESULTS:")
        for component, result in validation_results.items():
            if isinstance(result, dict) and 'status' in result:
                status_icon = "âœ…" if result['status'] == 'PASS' else "âŒ"
                print(f"  {status_icon} {component.replace('_', ' ').title()}: {result['status']}")
                if 'metrics' in result:
                    for metric, value in result['metrics'].items():
                        print(f"    â€¢ {metric}: {value}")
        
        validation_results['validation_timestamp'] = datetime.now().isoformat()
        validation_results['system_version'] = "4.0.0"
        
        with open("models/comprehensive_validation_report.json", "w") as f:
            json.dump(validation_results, f, indent=2)
        
        print(f"\nðŸ’¾ Validation report saved to: models/comprehensive_validation_report.json")
        
        if validation_results['overall_status'] in ['PASS', 'PARTIAL_PASS']:
            print("\nðŸš€ SYSTEM READY FOR PRODUCTION DEPLOYMENT")
            assert True, "System ready for production deployment"
        else:
            print("\nâš ï¸ SYSTEM REQUIRES FIXES BEFORE PRODUCTION DEPLOYMENT")
            assert False, f"System requires fixes: {validation_results}"
        
    except Exception as e:
        print(f"\nâŒ CRITICAL ERROR IN COMPREHENSIVE VALIDATION: {e}")
        import traceback
        traceback.print_exc()
        assert False, f"Critical error in comprehensive validation: {e}"

def test_ai_models_validation():
    """Test AI models and inference system"""
    try:
        manager = MedAIIntegrationManager()
        
        assert hasattr(manager, 'enhanced_models'), 'Manager does not have enhanced_models attribute'
        assert manager.enhanced_models and len(manager.enhanced_models) > 0, 'No AI models loaded in manager'
        
        test_image = np.random.rand(224, 224, 3) * 255
        test_image = test_image.astype(np.uint8)
        
        result = manager.analyze_image(test_image, "chest_xray")
        
        assert 'error' not in result, f"Inference failed: {result.get('error', 'Unknown error')}"
        
        required_fields = ['predicted_class', 'confidence']
        for field in required_fields:
            assert field in result, f"Missing required field: {field}"
        
        assert True, f"AI models validation passed with {len(manager.enhanced_models)} models loaded"
        
    except Exception as e:
        assert False, f"AI models validation failed: {str(e)}"

def test_clinical_monitoring_validation():
    """Test clinical monitoring dashboard"""
    try:
        dashboard = ClinicalMonitoringDashboard()
        
        test_metrics = {
            'sensitivity': 0.95,
            'specificity': 0.92,
            'auc': 0.88,
            'precision': 0.90,
            'f1_score': 0.92
        }
        
        dashboard.track_clinical_validation_metrics(test_metrics)
        
        html_dashboard = dashboard.generate_dashboard_html()
        assert len(html_dashboard) >= 1000, 'Dashboard HTML generation failed - output too short'
        
        metrics_json = dashboard.get_dashboard_metrics_json()
        metrics_data = json.loads(metrics_json)
        
        assert True, f"Clinical monitoring validation passed - dashboard size: {len(html_dashboard)}"
        
    except Exception as e:
        assert False, f"Clinical monitoring validation failed: {str(e)}"

def test_advanced_clinical_framework():
    """Test advanced clinical validation framework"""
    try:
        validator = AdvancedClinicalValidationFramework()
        
        mock_predictions = {
            'pneumonia': [0.8, 0.6, 0.9, 0.7, 0.5],
            'pleural_effusion': [0.2, 0.4, 0.1, 0.3, 0.5]
        }
        mock_ground_truth = {
            'pneumonia': [1, 0, 1, 1, 0],
            'pleural_effusion': [0, 1, 0, 0, 1]
        }
        
        study_metadata = {
            'study_name': 'Comprehensive Validation Test',
            'dataset_size': len(mock_predictions['pneumonia']),
            'validation_type': 'binary_classification'
        }
        
        validation_result = validator.conduct_comprehensive_validation_study(
            mock_predictions, mock_ground_truth, study_metadata
        )
        
        assert 'error' not in validation_result, f"Validation study failed: {validation_result.get('error', 'Unknown error')}"
        
        readiness = validator._assess_clinical_readiness(validation_result)
        
        assert True, f"Advanced clinical framework validation passed"
        
    except Exception as e:
        assert False, f"Advanced clinical framework validation failed: {str(e)}"

def test_production_configuration():
    """Test production configuration files"""
    try:
        with open("config/production.json", "r") as f:
            prod_config = json.load(f)
        
        required_sections = [
            'environment', 'deployment', 'server', 'database',
            'ai_models', 'clinical_monitoring', 'security'
        ]
        
        for section in required_sections:
            assert section in prod_config, f"Missing config section: {section}"
        
        assert prod_config['environment'] == 'production', 'Environment not set to production'
        assert prod_config['security']['authentication_required'], 'Authentication not required in production'
        
        assert True, f"Production configuration validation passed with {len(prod_config)} sections"
        
    except Exception as e:
        assert False, f"Production configuration validation failed: {str(e)}"

def test_docker_configuration():
    """Test Docker configuration files"""
    try:
        with open("Dockerfile", "r") as f:
            dockerfile_content = f.read()
        
        required_dockerfile_elements = [
            'FROM python:3.9-slim',
            'WORKDIR /app',
            'COPY requirements.txt',
            'RUN pip install',
            'EXPOSE'
        ]
        
        for element in required_dockerfile_elements:
            assert element in dockerfile_content, f"Missing Dockerfile element: {element}"
        
        with open("docker-compose.yml", "r") as f:
            compose_content = f.read()
        
        required_services = ['medai-app', 'medai-db', 'redis', 'nginx']
        for service in required_services:
            assert service in compose_content, f"Missing Docker service: {service}"
        
        assert True, f"Docker configuration validation passed"
        
    except Exception as e:
        assert False, f"Docker configuration validation failed: {str(e)}"

def test_performance_optimization():
    """Test performance optimization methods"""
    try:
        from medai_ml_pipeline import MLPipeline
        
        pipeline = MLPipeline(project_name="ValidationTest", experiment_name="performance_test")
        
        optimization_methods = [
            '_apply_medical_quantization',
            '_optimize_ensemble_weights',
            '_perform_final_clinical_validation'
        ]
        
        for method in optimization_methods:
            assert hasattr(pipeline, method), f"Missing optimization method: {method}"
        
        optimization_config = {'model_compression_ratio': 0.3}
        ensemble_result = pipeline._optimize_ensemble_weights(None, optimization_config)
        
        assert 'error' not in ensemble_result, f"Ensemble optimization failed: {ensemble_result.get('error', 'Unknown error')}"
        
        assert True, f"Performance optimization validation passed with {len(optimization_methods)} methods"
        
    except Exception as e:
        assert False, f"Performance optimization validation failed: {str(e)}"

def test_security_compliance():
    """Test security and compliance features"""
    try:
        with open("config/production.json", "r") as f:
            config = json.load(f)
        
        security_config = config.get('security', {})
        compliance_config = config.get('compliance', {})
        
        required_security = [
            'authentication_required',
            'rate_limiting',
            'cors',
            'audit_logging'
        ]
        
        for feature in required_security:
            assert feature in security_config, f"Missing security feature: {feature}"
            feature_config = security_config[feature]
            if isinstance(feature_config, dict):
                assert feature_config.get('enabled', False), f"Security feature not enabled: {feature}"
            elif isinstance(feature_config, bool):
                assert feature_config, f"Security feature not enabled: {feature}"
        
        required_compliance = [
            'hipaa_compliant',
            'gdpr_compliant',
            'audit_trail_enabled',
            'data_anonymization'
        ]
        
        for feature in required_compliance:
            assert compliance_config.get(feature, False), f"Compliance feature not enabled: {feature}"
        
        assert True, f"Security compliance validation passed with {len(required_security)} security features and {len(required_compliance)} compliance features"
        
    except Exception as e:
        assert False, f"Security compliance validation failed: {str(e)}"

if __name__ == "__main__":
    success = test_comprehensive_validation()
    sys.exit(0 if success else 1)
