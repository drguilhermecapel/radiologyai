{
  "original_model_path": "test_summary_model.h5",
  "original_size": 25992,
  "original_parameters": 1061,
  "optimization_types": [
    "quantization",
    "distillation"
  ],
  "target_accuracy_retention": 0.9,
  "optimized_models": {
    "quantized": {
      "error": "Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"sequential_4_1/conv2d_6_1/convolution@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"sequential_4_1/conv2d_6_1/convolution@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %6 = \"tf.Conv2D\"(%5, %3) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x222x222x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"sequential_4_1/conv2d_6_1/convolution@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/conv2d_6_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/conv2d_6_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %7 = \"tf.BiasAdd\"(%6, %2) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x222x222x32xf16>, tensor<32xf16>) -> tensor<?x222x222x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/conv2d_6_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"sequential_4_1/conv2d_6_1/Relu@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"sequential_4_1/conv2d_6_1/Relu@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %8 = \"tf.Relu\"(%7) {device = \"\"} : (tensor<?x222x222x32xf16>) -> tensor<?x222x222x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"sequential_4_1/conv2d_6_1/Relu@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"sequential_4_1/dense_8_1/MatMul@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"sequential_4_1/dense_8_1/MatMul@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %12 = \"tf.MatMul\"(%11, %0) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x32xf16>, tensor<5x32xf16>) -> tensor<?x5xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"sequential_4_1/dense_8_1/MatMul@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/dense_8_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/dense_8_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %13 = \"tf.BiasAdd\"(%12, %1) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x5xf16>, tensor<5xf16>) -> tensor<?x5xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"sequential_4_1/dense_8_1/BiasAdd@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Softmax:\", \"sequential_4_1/dense_8_1/Softmax@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Softmax' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Softmax:\", \"sequential_4_1/dense_8_1/Softmax@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %14 = \"tf.Softmax\"(%13) {device = \"\"} : (tensor<?x5xf16>) -> tensor<?x5xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Softmax:\", \"sequential_4_1/dense_8_1/Softmax@__inference_function_4407\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_4434\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: BiasAdd, Conv2D, MatMul, Relu, Softmax\nDetails:\n\ttf.BiasAdd(tensor<?x222x222x32xf16>, tensor<32xf16>) -> (tensor<?x222x222x32xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.BiasAdd(tensor<?x5xf16>, tensor<5xf16>) -> (tensor<?x5xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.Conv2D(tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x222x222x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.MatMul(tensor<?x32xf16>, tensor<5x32xf16>) -> (tensor<?x5xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n\ttf.Relu(tensor<?x222x222x32xf16>) -> (tensor<?x222x222x32xf16>) : {device = \"\"}\n\ttf.Softmax(tensor<?x5xf16>) -> (tensor<?x5xf16>) : {device = \"\"}\n\n<unknown>:0: note: see current operation: \n\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"input_layer_4\"]}], function_type = (tensor<?x224x224x3xf32>) -> tensor<?x5xf16>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n^bb0(%arg0: tensor<?x224x224x3xf32>):\n  %0 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<5x32xf16>}> : () -> tensor<5x32xf16>\n  %1 = \"arith.constant\"() <{value = dense<0.000000e+00> : tensor<5xf16>}> : () -> tensor<5xf16>\n  %2 = \"arith.constant\"() <{value = dense<0.000000e+00> : tensor<32xf16>}> : () -> tensor<32xf16>\n  %3 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n  %4 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n  %5 = \"tfl.cast\"(%arg0) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n  %6 = \"tf.Conv2D\"(%5, %3) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x222x222x32xf16>\n  %7 = \"tf.BiasAdd\"(%6, %2) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x222x222x32xf16>, tensor<32xf16>) -> tensor<?x222x222x32xf16>\n  %8 = \"tf.Relu\"(%7) {device = \"\"} : (tensor<?x222x222x32xf16>) -> tensor<?x222x222x32xf16>\n  %9 = \"tfl.cast\"(%8) : (tensor<?x222x222x32xf16>) -> tensor<?x222x222x32xf32>\n  %10 = \"tfl.mean\"(%9, %4) <{keep_dims = false}> : (tensor<?x222x222x32xf32>, tensor<2xi32>) -> tensor<?x32xf32>\n  %11 = \"tfl.cast\"(%10) : (tensor<?x32xf32>) -> tensor<?x32xf16>\n  %12 = \"tf.MatMul\"(%11, %0) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x32xf16>, tensor<5x32xf16>) -> tensor<?x5xf16>\n  %13 = \"tf.BiasAdd\"(%12, %1) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x5xf16>, tensor<5xf16>) -> tensor<?x5xf16>\n  %14 = \"tf.Softmax\"(%13) {device = \"\"} : (tensor<?x5xf16>) -> tensor<?x5xf16>\n  \"func.return\"(%14) : (tensor<?x5xf16>) -> ()\n}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_input_layer_4:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n"
    },
    "distilled": {
      "path": "test_summary_model_distilled.h5",
      "size_bytes": 284160,
      "size_reduction_percent": -993.2594644506003,
      "parameter_reduction": -5630.914231856739,
      "architecture": "Lightweight CNN",
      "clinical_validation": {
        "suitable_for_edge_deployment": true,
        "recommended_for_screening": false
      }
    }
  },
  "baseline_metrics": {
    "accuracy": 0.26,
    "precision": 0.0676,
    "recall": 0.26,
    "inference_time_ms": 93.38080883026123
  },
  "summary": {
    "total_optimizations_applied": 2,
    "successful_optimizations": 1,
    "failed_optimizations": 1,
    "best_optimization": null,
    "clinical_recommendations": []
  }
}